{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code allows to visualize the intermediate activations in a trained model. \n",
    "\n",
    "The representations learned by convnets are highly amenable to visualization, in large part because they are representations of visual concepts. Since 2013, a wide array of techniques have been developed for visualizing and interpreting these representations. \n",
    "\n",
    "Visualizing intermediate convnet outputs (\"intermediate activations\") helps to understand how successive convnet layers transform their input, and to get a first idea of the meaning of individual convnet filters.\n",
    "\n",
    "Visualizing intermediate activations consists in displaying the feature maps that are output by various convolution and pooling layers in a network, given a certain input. The output of a layer is often called its \"activation\", the output of the activation function. This gives a view into how an input is decomposed unto the different filters learned by the network. These feature maps we want to visualize have 3 dimensions: width, height, and depth (channels). Each channel encodes relatively independent features, so the proper way to visualize these feature maps is by independently plotting the contents of every channel, as a 2D image'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load libraries\n",
    "from keras.preprocessing import image\n",
    "import keras\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import models\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check keras version\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the trained model\n",
    "model = load_model('vgg16_custom.09-0.9650.h5')\n",
    "model.summary()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the image\n",
    "img_path = 'cxr (1).jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess the image into a 4D tensor\n",
    "img = image.load_img(img_path, target_size=(300,300))\n",
    "img_tensor = image.img_to_array(img)\n",
    "img_tensor = np.expand_dims(img_tensor, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model was trained on inputs that were preprocessed in the following way:\n",
    "img_tensor /= 255. # \n",
    "print(img_tensor.shape)\n",
    "\n",
    "#display the image\n",
    "plt.imshow(img_tensor[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extract the feature maps to look at, create a Keras model that takes batches of images as input, and outputs the activations of all convolution and pooling layers. Using the Keras class Model, the Model is instantiated using two arguments: an input tensor (or list of input tensors), and an output tensor (or list of output tensors). The resulting class is a Keras model, mapping the specified inputs to the specified outputs. It even allows for models with multiple outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display the numbers of the different layers in the trained model\n",
    "model.summary()\n",
    "for i, layer in enumerate(model.layers):\n",
    "   print(i, layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts the outputs of the top layers everything upto the GAP layer:\n",
    "layer_outputs = [layer.output for layer in model.layers[1:18]] #this model has 18 layers just before the GAP layer\n",
    "\n",
    "# Creates a model that will return these outputs, given the model input:\n",
    "activation_model = models.Model(inputs=model.input, outputs=layer_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When fed an image input, this model returns the values of the layer activations in the original model. This is a multi-output model. In the general case, a model could have any number of inputs and outputs. This one has one input and 18 outputs, one output per layer activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will return a list of Numpy arrays: one array per layer activation\n",
    "activations = activation_model.predict(img_tensor)\n",
    "\n",
    "# This is the activation of the first convolution layer for our image input:\n",
    "second_layer_activation = activations[1]\n",
    "print(second_layer_activation.shape) #(1, 300, 300, 64)\n",
    "\n",
    "#It's a 300*300 feature map with 64 channels. Visualizing the 60th and 10th channel:\n",
    "plt.matshow(second_layer_activation[0, :, :, 60], cmap='viridis')\n",
    "plt.show()\n",
    "\n",
    "#This channel appears to encode a diagonal edge detector. \n",
    "# 10th channel, since the specific filters learned by convolution layers are not deterministic.\n",
    "plt.matshow(second_layer_activation[0, :, :, 10], cmap='viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting a complete visualization of all the activations in the network. Extract and plot every channel in each of the 18 activation maps, (only that of the convolutional layers) and stack the results in one big image tensor, with channels stacked side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Names of the layers to have them as part of the plot\n",
    "layer_names = []\n",
    "for layer in model.layers[:18]:\n",
    "    layer_names.append(layer.name)\n",
    "\n",
    "images_per_row = 16 \n",
    "\n",
    "# Display feature maps\n",
    "for layer_name, layer_activation in zip(layer_names, activations):\n",
    "    \n",
    "    # This is the number of features in the feature map\n",
    "    n_features = layer_activation.shape[-1]\n",
    "\n",
    "    # The feature map has shape (1, size, size, n_features)\n",
    "    size = layer_activation.shape[1]\n",
    "\n",
    "    # Tile the activation channels in this matrix\n",
    "    n_cols = n_features // images_per_row\n",
    "    display_grid = np.zeros((size * n_cols, images_per_row * size))\n",
    "\n",
    "    # Tile each filter into this big horizontal grid\n",
    "    for col in range(n_cols):\n",
    "        for row in range(images_per_row):\n",
    "            channel_image = layer_activation[0,\n",
    "                                             :, :,\n",
    "                                             col * images_per_row + row]\n",
    "            # Post-process the feature to make it visually palatable\n",
    "            channel_image -= channel_image.mean()\n",
    "            channel_image /= channel_image.std()\n",
    "            channel_image *= 64 \n",
    "            channel_image += 128 \n",
    "            channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n",
    "            display_grid[col * size : (col + 1) * size,\n",
    "                         row * size : (row + 1) * size] = channel_image\n",
    "\n",
    "    # Display the grid\n",
    "    scale = 1. / size\n",
    "    plt.figure(figsize=(scale * display_grid.shape[1],\n",
    "                        scale * display_grid.shape[0]))\n",
    "    plt.title(layer_name)\n",
    "    plt.grid(False)\n",
    "    plt.imshow(display_grid, aspect='auto', cmap='viridis')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to note:\n",
    "The first layer acts as a collection of various edge detectors. At that stage, the activations are still retaining almost all of the information present in the initial picture.\n",
    "\n",
    "As we go higher-up, the activations become increasingly abstract and less visually interpretable. They start encoding higher-level concepts.\n",
    "\n",
    "Higher-up presentations carry increasingly less information about the visual contents of the image, and increasingly more information related to the class of the image.\n",
    "\n",
    "The sparsity of the activations is increasing with the depth of the layer: in the first layer, all filters are activated by the input image, but in the following layers more and more filters are blank. This means that the pattern encoded by the filter isn't found in the input image.\n",
    "\n",
    "Universal characteristic of the representations learned by deep neural networks: the features extracted by a layer get increasingly abstract with the depth of the layer. The activations of layers higher-up carry less and less information about the specific input being seen, and more and more information about the target, the image calss.\n",
    "\n",
    "A deep neural network effectively acts as an information distillation pipeline, with raw data going in, and getting repeatedly transformed so that irrelevant information gets filtered out (e.g. the specific visual appearance of the image)\n",
    "while useful information get magnified and refined (e.g. the class of the image).\n",
    "\n",
    "This is analogous to the way humans and animals perceive the world: after observing a scene for a few seconds, a human can remember which abstract objects were present in it (e.g. bicycle, tree) but could not remember the specific appearance of these objects. \n",
    "\n",
    "Human brain has learned to completely abstract its visual input, to transform it into high-level visual concepts while completely filtering out irrelevant visual details, making it tremendously difficult to remember how things around us actually look."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
